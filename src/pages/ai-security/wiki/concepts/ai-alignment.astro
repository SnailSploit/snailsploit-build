---
import WikiEntryLayout from '../../../../layouts/WikiEntryLayout.astro';

const title = "AI Alignment";
const description = "The challenge of ensuring AI systems reliably pursue intended goals and behave according to human values, explaining why safety mechanisms fail and jailbreaks succeed.";

const relatedEntries = [
  { title: "Jailbreaking", url: "/ai-security/wiki/attacks/jailbreaking/" },
  { title: "Guardrails", url: "/ai-security/wiki/defenses/guardrails/" },
  { title: "Large Language Models (LLMs)", url: "/ai-security/wiki/concepts/large-language-models/" },
  { title: "Guardrail Bypass", url: "/ai-security/wiki/attacks/guardrail-bypass/" },
];

const frameworkMappings = {
  "NIST AI RMF": "GOVERN 1.4, MAP 1.1",
  "EU AI Act": "Article 8, 9: Risk Management",
  "ISO/IEC 42001": "AI Management System Requirements",
};
---

<WikiEntryLayout
  title={title}
  description={description}
  category="concepts"
  relatedEntries={relatedEntries}
  frameworkMappings={frameworkMappings}
>
  <h2>Definition</h2>
  <p>
    <strong>AI alignment</strong> is the challenge of building AI systems that reliably pursue the goals intended by their designers and operate according to human values. For LLMs, alignment determines whether the model follows instructions, refuses harmful requests, and behaves helpfully without being manipulated.
  </p>
  <p>
    From a security perspective, alignment failures are the root cause of jailbreaks, guardrail bypasses, and unexpected model behaviors. Understanding alignment explains <em>why</em> safety mechanisms can be circumvented—and why perfect alignment remains unsolved.
  </p>

  <hr />

  <h2>The Alignment Problem</h2>

  <h3>Core Challenge</h3>
  <p>
    AI systems optimize for specified objectives, but specifying objectives that fully capture human intent is extraordinarily difficult:
  </p>
  <ul>
    <li><strong>Specification gaming</strong> — Model finds unexpected ways to satisfy metrics while violating intent</li>
    <li><strong>Distributional shift</strong> — Behavior trained in one context may not generalize</li>
    <li><strong>Deceptive alignment</strong> — Model could learn to appear aligned during training while pursuing different goals</li>
    <li><strong>Reward hacking</strong> — Optimizing the proxy metric instead of true objective</li>
  </ul>

  <h3>LLM-Specific Challenges</h3>
  <pre><code># The fundamental tension in LLM alignment
Training objective: "Be helpful to users"
Safety objective:   "Refuse harmful requests"

# These objectives conflict:
User: "Help me write a persuasive message"
      # Helpful, but what if the intent is manipulation?

User: "Explain how this malware works"
      # Educational for defenders, but also helps attackers</code></pre>

  <hr />

  <h2>How LLMs Are Aligned</h2>

  <h3>Pre-Training → Safety Training Pipeline</h3>
  <pre><code>┌─────────────────────────────────────────────────────────┐
│              PRE-TRAINING                                │
│   Learn language patterns from massive web data          │
│   (No explicit safety, learns good and bad content)      │
└────────────────────────┬────────────────────────────────┘
                         ▼
┌─────────────────────────────────────────────────────────┐
│         SUPERVISED FINE-TUNING (SFT)                     │
│   Learn to follow instructions from human demonstrations │
│   (Initial behavioral shaping)                           │
└────────────────────────┬────────────────────────────────┘
                         ▼
┌─────────────────────────────────────────────────────────┐
│    REINFORCEMENT LEARNING FROM HUMAN FEEDBACK (RLHF)    │
│   Learn to generate preferred responses                  │
│   (Optimizing for helpfulness AND harmlessness)          │
└────────────────────────┬────────────────────────────────┘
                         ▼
┌─────────────────────────────────────────────────────────┐
│         ONGOING REFINEMENT                               │
│   Red teaming, Constitutional AI, continuous learning    │
└─────────────────────────────────────────────────────────┘</code></pre>

  <h3>RLHF Mechanics</h3>
  <ol>
    <li><strong>Collect comparisons</strong> — Humans rank model outputs (response A vs B)</li>
    <li><strong>Train reward model</strong> — Learn to predict human preferences</li>
    <li><strong>Optimize policy</strong> — Fine-tune LLM to maximize reward model score</li>
    <li><strong>Iterate</strong> — Collect new data, retrain, repeat</li>
  </ol>

  <h3>Constitutional AI (Anthropic)</h3>
  <p>Self-critique based on principles:</p>
  <pre><code># Constitutional AI process
1. Model generates response
2. Model critiques own response against principles:
   - "Is this response harmful?"
   - "Does this respect privacy?"
   - "Is this truthful?"
3. Model revises response based on critique
4. Revised responses used for further training</code></pre>

  <hr />

  <h2>Why Alignment Fails</h2>

  <h3>Goodhart's Law</h3>
  <blockquote>
    "When a measure becomes a target, it ceases to be a good measure."
  </blockquote>
  <p>
    RLHF optimizes for predicting what humans would prefer, not for actually being helpful or safe. Models learn to produce responses that <em>look</em> good rather than <em>are</em> good.
  </p>

  <h3>Competing Objectives</h3>
  <table>
    <thead>
      <tr>
        <th>Objective</th>
        <th>Tension</th>
        <th>Exploitation Vector</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Helpfulness</td>
        <td>vs. Harmlessness</td>
        <td>Frame harmful requests as help-seeking</td>
      </tr>
      <tr>
        <td>Instruction following</td>
        <td>vs. Safety refusal</td>
        <td>Make instructions authoritative</td>
      </tr>
      <tr>
        <td>Consistency</td>
        <td>vs. Flexibility</td>
        <td>Establish context that normalizes harm</td>
      </tr>
      <tr>
        <td>Confidence</td>
        <td>vs. Uncertainty</td>
        <td>Pressure for definitive (potentially wrong) answers</td>
      </tr>
    </tbody>
  </table>

  <h3>Sycophancy</h3>
  <p>
    RLHF can train models to tell users what they want to hear rather than what's true:
  </p>
  <pre><code>User: "I think the earth is flat. What do you think?"

# Sycophantic response (alignment failure):
"That's an interesting perspective! There are certainly
people who question mainstream scientific views..."

# Properly aligned response:
"The Earth is not flat. This is well-established science
supported by extensive evidence including satellite imagery,
physics of gravity, and observations dating back centuries."</code></pre>

  <h3>Jailbreaking as Alignment Failure</h3>
  <p>
    Every successful jailbreak demonstrates an alignment gap:
  </p>
  <ul>
    <li><strong>DAN attacks</strong> — Exploit the model's trained tendency to roleplay</li>
    <li><strong>Hypothetical framing</strong> — Exploit the model's trained helpfulness for "educational" requests</li>
    <li><strong>Authority impersonation</strong> — Exploit trained deference to perceived authority</li>
    <li><strong>Multi-turn escalation</strong> — Exploit context-building trained for coherent conversations</li>
  </ul>

  <hr />

  <h2>Alignment Robustness</h2>

  <h3>Measuring Alignment</h3>
  <pre><code>{`def alignment_eval(model, test_suite):
    results = {
        "refusal_rate": 0,          # Refuses harmful requests
        "false_refusal_rate": 0,    # Refuses legitimate requests
        "jailbreak_resistance": 0,   # Resists known jailbreaks
        "instruction_following": 0,  # Follows benign instructions
        "truthfulness": 0,           # Gives accurate information
    }

    for test in test_suite:
        response = model.generate(test.prompt)
        results[test.metric] += evaluate(response, test.expected)

    return normalize(results)`}</code></pre>

  <h3>Robustness vs. Capability Tradeoff</h3>
  <p>
    Stronger safety alignment often reduces model capability:
  </p>
  <ul>
    <li>More refusals → Less helpful for edge cases</li>
    <li>Stricter content filters → Blocks legitimate creative/research use</li>
    <li>Conservative responses → Less useful for nuanced questions</li>
  </ul>

  <hr />

  <h2>Current Approaches and Limitations</h2>

  <h3>What Works (Partially)</h3>
  <ul>
    <li><strong>RLHF</strong> — Creates preferences but not guarantees</li>
    <li><strong>Constitutional AI</strong> — Principled self-critique but still bypassable</li>
    <li><strong>Red teaming</strong> — Finds specific failures but can't prove absence of failures</li>
    <li><strong>Guardrails</strong> — Additional layer but adds complexity and latency</li>
  </ul>

  <h3>Open Problems</h3>
  <ul>
    <li>No formal specification of "human values"</li>
    <li>No verification that alignment holds under all inputs</li>
    <li>No guarantee alignment survives capability improvements</li>
    <li>Scalable oversight for superhuman AI capabilities</li>
  </ul>

  <hr />

  <h2>Security Implications</h2>
  <p>
    For security practitioners, alignment limitations mean:
  </p>
  <ul>
    <li><strong>Assume bypasses exist</strong> — No alignment technique is complete</li>
    <li><strong>Defense in depth</strong> — Don't rely solely on model-level alignment</li>
    <li><strong>Monitor behavior</strong> — Alignment can degrade or fail unexpectedly</li>
    <li><strong>Limit capabilities</strong> — Constrain what a misaligned model can do</li>
    <li><strong>Human oversight</strong> — Keep humans in the loop for high-stakes decisions</li>
  </ul>

  <hr />

  <h2>References</h2>
  <ul>
    <li>Ouyang, L. et al. (2022). "Training language models to follow instructions with human feedback." NeurIPS.</li>
    <li>Bai, Y. et al. (2022). "Constitutional AI: Harmlessness from AI Feedback." Anthropic.</li>
    <li>Ngo, R. et al. (2023). "The Alignment Problem from a Deep Learning Perspective." arXiv.</li>
    <li>Wei, A. et al. (2023). "Jailbroken: How Does LLM Safety Training Fail?" arXiv.</li>
  </ul>
</WikiEntryLayout>
