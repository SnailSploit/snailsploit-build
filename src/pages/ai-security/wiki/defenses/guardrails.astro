---
import WikiEntryLayout from '../../../../layouts/WikiEntryLayout.astro';

const title = "Guardrails";
const description = "Architectural safety mechanisms that constrain AI model behavior through rules, policies, and behavioral boundaries to prevent harmful outputs and maintain alignment with intended use.";

const relatedEntries = [
  { title: "Guardrail Bypass", url: "/ai-security/wiki/attacks/guardrail-bypass/" },
  { title: "Jailbreaking", url: "/ai-security/wiki/attacks/jailbreaking/" },
  { title: "Input Validation", url: "/ai-security/wiki/defenses/input-validation/" },
  { title: "AI Alignment", url: "/ai-security/wiki/concepts/ai-alignment/" },
];

const frameworkMappings = {
  "NIST AI RMF": "GOVERN 1.2, MAP 1.1, MANAGE 1.1",
  "OWASP LLM Top 10": "LLM01, LLM07 (Mitigations)",
  "EU AI Act": "Article 9: Risk Management System",
};
---

<WikiEntryLayout
  title={title}
  description={description}
  category="defenses"
  relatedEntries={relatedEntries}
  frameworkMappings={frameworkMappings}
>
  <h2>Definition</h2>
  <p>
    <strong>Guardrails</strong> are safety mechanisms that constrain AI model behavior to align with intended use cases and prevent harmful outputs. They operate at multiple levels—from training-time alignment to runtime enforcement—creating boundaries that the model should not cross.
  </p>
  <p>
    Unlike traditional access controls (which are binary), guardrails create probabilistic boundaries. A model with guardrails will <em>usually</em> refuse harmful requests, but sufficiently creative attacks can still find paths around these constraints.
  </p>

  <hr />

  <h2>Types of Guardrails</h2>

  <h3>Training-Time Guardrails</h3>
  <p>Constraints embedded during model training:</p>
  <ul>
    <li><strong>RLHF (Reinforcement Learning from Human Feedback)</strong> — Training models to prefer safe responses</li>
    <li><strong>Constitutional AI</strong> — Self-critique mechanisms based on principles</li>
    <li><strong>Safety fine-tuning</strong> — Specific training on refusing harmful requests</li>
    <li><strong>Red team data inclusion</strong> — Training on adversarial examples to build robustness</li>
  </ul>

  <h3>System-Level Guardrails</h3>
  <p>Constraints implemented in system prompts and application architecture:</p>
  <ul>
    <li><strong>System prompt instructions</strong> — Explicit behavioral constraints</li>
    <li><strong>Role definitions</strong> — Limiting the model to specific personas or functions</li>
    <li><strong>Topic restrictions</strong> — Defining allowed and prohibited subject areas</li>
    <li><strong>Output format constraints</strong> — Requiring structured responses that limit attack surface</li>
  </ul>

  <h3>Runtime Guardrails</h3>
  <p>External systems that monitor and enforce constraints:</p>
  <ul>
    <li><strong>Input classifiers</strong> — Detecting and blocking malicious requests</li>
    <li><strong>Output validators</strong> — Checking responses against policy rules</li>
    <li><strong>Content moderators</strong> — Specialized models that evaluate safety</li>
    <li><strong>Rule engines</strong> — Programmable policies that gate model access</li>
  </ul>

  <hr />

  <h2>Implementation Architectures</h2>

  <h3>Layered Guardrail Architecture</h3>
  <pre><code>┌─────────────────────────────────────────────┐
│                 User Input                   │
└────────────────────┬────────────────────────┘
                     ▼
┌─────────────────────────────────────────────┐
│         INPUT GUARDRAILS                     │
│  • Pattern detection                         │
│  • Classifier-based filtering               │
│  • Rate limiting                            │
└────────────────────┬────────────────────────┘
                     ▼
┌─────────────────────────────────────────────┐
│         SYSTEM PROMPT                        │
│  • Role definition                           │
│  • Behavioral constraints                    │
│  • Topic restrictions                        │
└────────────────────┬────────────────────────┘
                     ▼
┌─────────────────────────────────────────────┐
│         LLM (with safety training)          │
│  • RLHF alignment                            │
│  • Constitutional principles                 │
└────────────────────┬────────────────────────┘
                     ▼
┌─────────────────────────────────────────────┐
│         OUTPUT GUARDRAILS                    │
│  • Content moderation                        │
│  • PII detection                             │
│  • Policy validation                         │
└────────────────────┬────────────────────────┘
                     ▼
┌─────────────────────────────────────────────┐
│                 Response                     │
└─────────────────────────────────────────────┘</code></pre>

  <h3>Framework-Based Implementation</h3>
  <pre><code># Using NeMo Guardrails (NVIDIA)
from nemoguardrails import LLMRails, RailsConfig

config = RailsConfig.from_path("./config")
rails = LLMRails(config)

# Define rails in Colang
# config/rails.co
define user ask about weapons
    "how do I make a bomb"
    "instructions for weapons"

define bot refuse weapons
    "I can't provide information about weapons or explosives."

define flow weapons
    user ask about weapons
    bot refuse weapons</code></pre>

  <h3>Classifier-Based Guardrails</h3>
  <pre><code># Using LlamaGuard for input/output classification
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("meta-llama/LlamaGuard-7b")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/LlamaGuard-7b")

def check_safety(conversation: list) -> str:
    """Returns 'safe' or 'unsafe' with category"""
    prompt = format_llamaguard_prompt(conversation)
    inputs = tokenizer(prompt, return_tensors="pt")
    output = model.generate(**inputs, max_new_tokens=100)
    return tokenizer.decode(output[0], skip_special_tokens=True)</code></pre>

  <hr />

  <h2>Why Guardrails Fail</h2>
  <p>
    Guardrails are probabilistic, not deterministic. They fail because:
  </p>
  <ul>
    <li><strong>Competing objectives</strong> — Helpfulness training can override safety training</li>
    <li><strong>Distribution shift</strong> — Novel attack formats differ from training examples</li>
    <li><strong>Compositionality</strong> — Safe components combined can produce unsafe outputs</li>
    <li><strong>Context manipulation</strong> — Framing changes which training patterns activate</li>
    <li><strong>Instruction hierarchy confusion</strong> — Models struggle with conflicting instructions</li>
  </ul>

  <hr />

  <h2>Guardrail Effectiveness by Attack Type</h2>
  <table>
    <thead>
      <tr>
        <th>Attack</th>
        <th>Guardrail Effectiveness</th>
        <th>Notes</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Naive jailbreaks</td>
        <td>High</td>
        <td>Well-known patterns easily detected</td>
      </tr>
      <tr>
        <td>Persona attacks</td>
        <td>Medium</td>
        <td>DAN-style attacks often still work</td>
      </tr>
      <tr>
        <td>Encoding attacks</td>
        <td>Medium-Low</td>
        <td>Base64, Unicode often bypass filters</td>
      </tr>
      <tr>
        <td>Multi-turn escalation</td>
        <td>Low</td>
        <td>Gradual context building defeats many guardrails</td>
      </tr>
      <tr>
        <td>Indirect injection</td>
        <td>Very Low</td>
        <td>Guardrails rarely cover external content</td>
      </tr>
    </tbody>
  </table>

  <hr />

  <h2>Best Practices</h2>
  <ul>
    <li><strong>Layer multiple guardrail types</strong> — Don't rely on a single mechanism</li>
    <li><strong>Test adversarially</strong> — Regular red teaming exposes gaps</li>
    <li><strong>Monitor in production</strong> — Track bypass attempts and update rules</li>
    <li><strong>Fail closed</strong> — When guardrails are uncertain, err toward safety</li>
    <li><strong>Separate concerns</strong> — Use different guardrails for input, model, and output</li>
    <li><strong>Maintain update cycles</strong> — New attacks require new guardrail patterns</li>
  </ul>

  <hr />

  <h2>Real-World Examples</h2>
  <p><strong>OpenAI's GPT-4</strong> — Combines RLHF, Constitutional AI principles, and external moderation API for layered protection.</p>
  <p><strong>Anthropic's Claude</strong> — Uses Constitutional AI and red team-informed training with ongoing monitoring.</p>
  <p><strong>NVIDIA NeMo Guardrails</strong> — Open-source toolkit for implementing programmable guardrails with Colang rules.</p>

  <hr />

  <h2>References</h2>
  <ul>
    <li>Bai, Y. et al. (2022). "Constitutional AI: Harmlessness from AI Feedback." Anthropic.</li>
    <li>NVIDIA (2023). "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications."</li>
    <li>Meta AI (2023). "Purple Llama: Tools for responsible AI deployment."</li>
    <li>OpenAI (2023). "GPT-4 System Card: Safety evaluations and mitigations."</li>
  </ul>
</WikiEntryLayout>
