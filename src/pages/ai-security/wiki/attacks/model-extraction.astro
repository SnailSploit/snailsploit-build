---
import WikiEntryLayout from '../../../../layouts/WikiEntryLayout.astro';

const title = "Model Extraction";
const description = "An attack that steals the functionality of a machine learning model through systematic querying, allowing attackers to replicate proprietary models without access to training data or weights.";

const relatedEntries = [
  { title: "System Prompt Extraction", url: "/ai-security/wiki/attacks/system-prompt-extraction/" },
  { title: "Adversarial AI", url: "/ai-security/wiki/concepts/adversarial-ai/" },
];

const frameworkMappings = {
  "MITRE ATLAS": "AML.T0024: Model Theft",
  "OWASP LLM Top 10": "LLM10: Model Theft",
  "AATMF": "ME-* (Model Extraction category)",
};
---

<WikiEntryLayout
  title={title}
  description={description}
  category="attacks"
  relatedEntries={relatedEntries}
  frameworkMappings={frameworkMappings}
>
  <h2>Definition</h2>
  <p>
    <strong>Model extraction</strong> (also called model stealing) allows an attacker to create a functional copy of a machine learning model by systematically querying it and training a surrogate model on the responses. The attacker doesn't need access to training data, model weights, or architecture—just query access.
  </p>

  <hr />

  <h2>How It Works</h2>
  <ol>
    <li><strong>Query the target</strong> — Send inputs and collect outputs</li>
    <li><strong>Build dataset</strong> — Create input-output pairs from responses</li>
    <li><strong>Train surrogate</strong> — Train a new model on collected data</li>
    <li><strong>Refine</strong> — Iteratively improve with targeted queries</li>
  </ol>

  <hr />

  <h2>Attack Variants</h2>

  <h3>Functionally Equivalent Extraction</h3>
  <p>Creating a model that produces identical outputs for any input.</p>

  <h3>Fidelity Extraction</h3>
  <p>Approximating model behavior to acceptable accuracy for attacker goals.</p>

  <h3>Decision Boundary Extraction</h3>
  <p>Learning the classification boundaries without full model replication.</p>

  <hr />

  <h2>Why It Matters</h2>
  <ul>
    <li><strong>IP theft</strong> — Months of training work stolen through API access</li>
    <li><strong>Attack enablement</strong> — Extracted models enable white-box attacks</li>
    <li><strong>Competitive advantage</strong> — Replicate competitor capabilities</li>
    <li><strong>Bypass restrictions</strong> — Use extracted model without rate limits</li>
  </ul>

  <hr />

  <h2>Detection</h2>
  <ul>
    <li>Monitor for unusual query patterns (systematic, high-volume)</li>
    <li>Detect queries designed to map decision boundaries</li>
    <li>Identify synthetic-looking input distributions</li>
    <li>Track API usage anomalies per user/organization</li>
  </ul>

  <hr />

  <h2>Defenses</h2>
  <ul>
    <li><strong>Rate limiting</strong> — Restrict query volume</li>
    <li><strong>Query diversity requirements</strong> — Flag repetitive patterns</li>
    <li><strong>Output perturbation</strong> — Add noise to responses</li>
    <li><strong>Watermarking</strong> — Embed detectable signatures</li>
    <li><strong>API monitoring</strong> — Behavioral analysis of usage</li>
  </ul>

  <hr />

  <h2>References</h2>
  <ul>
    <li>Tramer, F. et al. (2016). "Stealing Machine Learning Models via Prediction APIs."</li>
    <li>Jagielski, M. et al. (2020). "High Accuracy and High Fidelity Extraction of Neural Networks."</li>
  </ul>
</WikiEntryLayout>
