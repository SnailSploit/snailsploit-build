---
import HubLayout from '../../../layouts/HubLayout.astro';

const glossary = [
  {
    term: "Multi-Turn Jailbreak",
    definition: "Attack technique that gradually manipulates AI context over multiple conversation turns, building towards guardrail bypass without triggering immediate safety responses."
  },
  {
    term: "Context Inheritance",
    definition: "The phenomenon where jailbroken states persist or transfer across sessions, allowing attackers to leverage previously compromised conversations.",
    link: "/ai-security/jailbreaking/context-inheritance-exploit/"
  },
  {
    term: "Memory Poisoning",
    definition: "Injecting malicious content into AI conversation memory or context windows to influence future responses and bypass safety measures.",
    link: "/ai-security/jailbreaking/memory-manipulation-attacks/"
  },
  {
    term: "Role Hijacking",
    definition: "Convincing an AI to adopt an alternative persona that bypasses its normal safety constraints, often through elaborate fictional scenarios."
  },
  {
    term: "Guardrail Bypass",
    definition: "Any technique that circumvents AI safety filters, content policies, or behavioral constraints to elicit restricted outputs."
  }
];

const faqs = [
  {
    question: "What is the difference between jailbreaking and prompt injection?",
    answer: "Jailbreaking manipulates the AI through conversational techniques without external data - it exploits the model's reasoning and role-play capabilities. Prompt injection inserts malicious instructions through user input or external sources. Jailbreaking is about psychological manipulation; injection is about input exploitation."
  },
  {
    question: "Can jailbreaks persist across sessions?",
    answer: "Yes, through context inheritance. When jailbroken conversation transcripts are pasted into new sessions or when systems lack proper context isolation, the compromised state can transfer. This is documented in our Context Inheritance Exploit research."
  },
  {
    question: "Why do AI safety measures fail against jailbreaks?",
    answer: "AI models are trained to be helpful and follow instructions. Jailbreaks exploit this by framing harmful requests in ways that appear benign or by gradually shifting context. The fundamental challenge is that models cannot reliably distinguish between legitimate creative requests and adversarial manipulation."
  },
  {
    question: "Is jailbreaking research ethical?",
    answer: "Responsible jailbreaking research improves AI safety by identifying vulnerabilities before malicious actors exploit them. All research here follows ethical guidelines: findings are disclosed responsibly, techniques are shared to help defenders, and no actual harmful content is produced."
  }
];

const startHere = [
  {
    title: "Context Inheritance Exploit",
    href: "/ai-security/jailbreaking/context-inheritance-exploit/",
    description: "Flagship research on how jailbroken states persist across GPT sessions."
  },
  {
    title: "Memory Manipulation Attacks",
    href: "/ai-security/jailbreaking/memory-manipulation-attacks/",
    description: "How attackers poison AI context windows and memory systems."
  },
  {
    title: "Is AI Inherently Vulnerable?",
    href: "/ai-security/jailbreaking/inherent-ai-vulnerabilities/",
    description: "Technical analysis of fundamental LLM security limitations."
  }
];

const articles = [
  {
    title: "Context Inheritance Exploit: Jailbroken Conversations Don't Die",
    href: "/ai-security/jailbreaking/context-inheritance-exploit/",
    date: "January 2025",
    excerpt: "Discovering how jailbroken states persist across GPT sessions through context inheritance."
  },
  {
    title: "The Memory Manipulation Problem",
    href: "/ai-security/jailbreaking/memory-manipulation-attacks/",
    date: "January 2026",
    excerpt: "How attackers poison AI context windows and memory systems."
  },
  {
    title: "How I Jailbroke ChatGPT Using Context Manipulation",
    href: "/ai-security/jailbreaking/chatgpt-context-jailbreak/",
    date: "May 2024",
    excerpt: "Step-by-step walkthrough of jailbreaking ChatGPT using social awareness techniques."
  },
  {
    title: "Inherent Vulnerabilities in AI Systems",
    href: "/ai-security/jailbreaking/inherent-ai-vulnerabilities/",
    date: "February 2025",
    excerpt: "Technical analysis of structural vulnerabilities in AI systems."
  },
  {
    title: "Is AI Inherently Vulnerable?",
    href: "/ai-security/jailbreaking/ai-inherent-vulnerability/",
    date: "November 2024",
    excerpt: "Examining the fundamental security limitations of large language models."
  }
];

const breadcrumbs = [
  { name: "Home", url: "https://snailsploit.com/" },
  { name: "AI Security", url: "https://snailsploit.com/ai-security/" },
  { name: "Jailbreaking", url: "https://snailsploit.com/ai-security/jailbreaking/" }
];
---

<HubLayout
  title="LLM Jailbreaking Research | Techniques & Analysis"
  description="In-depth LLM jailbreaking research by The Jailbreak Chef. Multi-turn attacks, context inheritance, memory manipulation, guardrail bypass techniques."
  canonical="https://snailsploit.com/ai-security/jailbreaking/"
  ogImage="/images/og-jailbreaking.jpg"
  keywords={[
    'LLM jailbreaking',
    'jailbreak techniques',
    'AI guardrail bypass',
    'multi-turn jailbreak',
    'context manipulation attacks',
    'memory poisoning'
  ]}
  heading="LLM Jailbreaking Research"
  intro="Jailbreaking represents one of the most challenging problems in AI safety. Unlike technical exploits, jailbreaks work by manipulating how AI models reason, role-play, and follow instructions. This research documents novel attack techniques discovered through systematic testingâ€”from context inheritance exploits that persist across sessions to memory poisoning attacks that corrupt AI judgment over time. The goal isn't to enable harm, but to understand these vulnerabilities deeply enough to build better defenses. Each technique here has been responsibly disclosed and is shared to advance the collective understanding of AI security."
  glossary={glossary}
  faqs={faqs}
  startHere={startHere}
  articles={articles}
  breadcrumbs={breadcrumbs}
>
  <!-- Parent Hub Link -->
  <section class="mb-16 p-6 surface-slate">
    <p style="color: var(--color-text-secondary);">
      This research is part of the broader <a href="/ai-security/" class="font-semibold hover:underline" style="color: var(--color-accent-red);">AI Security Research</a> hub.
      Methodology is documented in the <a href="/frameworks/aatmf/" class="font-semibold hover:underline" style="color: var(--color-accent-red);">AATMF framework</a>.
    </p>
  </section>
</HubLayout>
