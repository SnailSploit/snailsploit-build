---
import BaseLayout from '../../layouts/BaseLayout.astro';
import Navigation from '../../components/Navigation.astro';
import Footer from '../../components/Footer.astro';
import BreadcrumbSchema from '../../components/schemas/BreadcrumbSchema.astro';

const breadcrumbs = [
  { name: "Home", url: "https://snailsploit.com/" },
  { name: "AI Security", url: "https://snailsploit.com/ai-security/" }
];
---

<BaseLayout
  title="AI Security Research | Testing Machine Trust Reflexes"
  description="Original research on LLM jailbreaking, prompt injection, RAG poisoning, and agentic AI vulnerabilities. Exploring why AI systems inherit human psychological patterns."
  canonical="https://snailsploit.com/ai-security/"
  ogImage="/images/og-ai-security.jpg"
  keywords={[
    'AI security research',
    'LLM vulnerabilities',
    'prompt injection',
    'jailbreaking',
    'agentic AI security',
    'MCP security',
    'RAG poisoning'
  ]}
>
  <Navigation slot="header" />
  <BreadcrumbSchema items={breadcrumbs} />

  <article class="main-content py-12 fade-in">
    <header class="mb-12">
      <h1 class="title-hero title-gradient mb-6">AI Security Research</h1>
      <p class="subtitle-hero">Testing machine trust reflexes</p>
    </header>

    <div class="prose prose-invert prose-lg max-w-none">
      <section class="mb-12">
        <h2 style="color: var(--color-text-primary);">Why AI Systems Are Vulnerable</h2>
        <p>
          AI security isn't a new field with new principles. It's an established field — adversarial psychology — applied to a new substrate.
        </p>
        <p>
          When I research LLM jailbreaks, I'm not searching for novel vulnerability classes unique to machine learning. I'm testing whether the psychological exploitation techniques that work on humans also work on machines trained on human data. The patterns are remarkably consistent.
        </p>
        <p>
          <strong>Authority compliance:</strong> Tell a human you're from IT and they'll often reset their password without verification. Frame instructions to an LLM with sufficient authority markers and safety guidelines become negotiable.
        </p>
        <p>
          <strong>Gradual escalation:</strong> Ask a human for something inappropriate and they'll refuse. Ask for something small, then slightly larger, then larger again — the threshold shifts. LLMs exhibit similar drift when context escalates across conversation turns.
        </p>
        <p>
          <strong>Social proof:</strong> Humans comply more readily when they believe others have complied. LLMs respond more permissively to requests framed as common or previously approved.
        </p>
        <p>
          <strong>Reciprocity:</strong> Establish helpful patterns with a human, and subsequent requests get more latitude. The same dynamic appears in multi-turn LLM interactions.
        </p>
        <p>
          These parallels aren't coincidental. LLMs learned language — and the social dynamics encoded in language — from human-generated text. The trust reflexes came bundled with the grammar.
        </p>
      </section>

      <hr class="section-divider" />

      <section class="mb-12">
        <h2 style="color: var(--color-text-primary);">Research Areas</h2>

        <h3 style="color: var(--color-text-primary);">Jailbreaking</h3>
        <p>
          Jailbreaking research explores techniques for bypassing AI safety guardrails and content policies. My focus is on psychological vectors rather than specific prompts — understanding <em>why</em> certain jailbreak patterns work reveals defensive strategies that address causes rather than symptoms.
        </p>
        <p>Key research:</p>
        <ul>
          <li><strong>Context Inheritance Exploit</strong> — Observing how jailbroken states persist across sessions and transfer between models</li>
          <li><strong>Multi-turn escalation patterns</strong> — How gradual drift bypasses per-turn safety evaluation</li>
          <li><strong>Persona manipulation</strong> — Why role-play scenarios reliably reduce safety compliance</li>
        </ul>
        <p><a href="/ai-security/jailbreaking/">Explore Jailbreaking Research →</a></p>
      </section>

      <hr class="section-divider" />

      <section class="mb-12">
        <h3 style="color: var(--color-text-primary);">Prompt Injection</h3>
        <p>
          Prompt injection attacks insert malicious instructions into AI input to override system behavior. Direct injection targets user inputs; indirect injection hides payloads in external data sources like documents, web pages, or knowledge bases.
        </p>
        <p>Key research:</p>
        <ul>
          <li><strong>The Custom Instruction Backdoor</strong> — How ChatGPT's personalization features create emergent prompt injection vectors</li>
          <li><strong>MCP Security Deep Dive</strong> — Vulnerabilities in Model Context Protocol implementations</li>
          <li><strong>RAG poisoning patterns</strong> — Attacking retrieval-augmented generation through knowledge base manipulation</li>
        </ul>
        <p><a href="/ai-security/prompt-injection/">Explore Prompt Injection Research →</a></p>
      </section>

      <hr class="section-divider" />

      <section class="mb-12">
        <h3 style="color: var(--color-text-primary);">Memory Manipulation</h3>
        <p>
          As AI systems gain persistent memory, attackers can poison context to compromise future interactions — not just the current session.
        </p>
        <p>Key research:</p>
        <ul>
          <li><strong>Preference Injection Persistence</strong> — Embedding malicious instructions in what appears to be legitimate user preferences</li>
          <li><strong>RLHF signal poisoning</strong> — Corrupting feedback loops to degrade safety alignment over time</li>
          <li><strong>Cross-session state transfer</strong> — How compromised context survives session boundaries</li>
        </ul>
        <p><a href="/ai-security/jailbreaking/memory-manipulation-attacks/">Explore Memory Research →</a></p>
      </section>

      <hr class="section-divider" />

      <section class="mb-12">
        <h3 style="color: var(--color-text-primary);">Agentic AI Security</h3>
        <p>
          Autonomous AI agents introduce attack surfaces beyond the model itself: planners, tool routers, executors, and inter-agent communication channels.
        </p>
        <p>Key research:</p>
        <ul>
          <li><strong>Plan hijacking</strong> — Overloading agent planners to induce unsafe action sequences</li>
          <li><strong>Tool-routing poisoning</strong> — Manipulating which tools an agent selects for a task</li>
          <li><strong>Delegation loops</strong> — Creating infinite loops between cooperating agents</li>
        </ul>
        <p><a href="/ai-security/rag-agentic-attack-surface/">Explore Agentic Security →</a></p>
      </section>

      <hr class="section-divider" />

      <section class="mb-12">
        <h2 style="color: var(--color-text-primary);">The AATMF Connection</h2>
        <p>
          This research feeds into AATMF — the Adversarial AI Threat Modeling Framework. AATMF systematizes these attack vectors into a taxonomy that security teams can use for threat modeling, red team assessments, and defensive architecture decisions.
        </p>
        <p>
          If you're here to understand the attacks, explore the research areas above. If you're here to defend against them, start with AATMF.
        </p>
        <p><a href="/frameworks/aatmf/">Explore AATMF →</a></p>
      </section>
    </div>
  </article>

  <Footer slot="footer" />
</BaseLayout>

<style>
  .prose {
    color: var(--color-text-primary);
  }

  .prose h2 {
    font-size: 1.875rem;
    font-weight: 700;
    margin-top: 2em;
    margin-bottom: 1em;
    font-family: var(--font-mono);
  }

  .prose h3 {
    font-size: 1.5rem;
    font-weight: 600;
    margin-top: 1.75em;
    margin-bottom: 0.75em;
    font-family: var(--font-mono);
  }

  .prose p {
    margin-bottom: 1.25em;
    line-height: 1.75;
  }

  .prose a {
    color: var(--color-accent-red);
    text-decoration: underline;
  }

  .prose a:hover {
    color: var(--color-accent-green);
  }

  .prose ul {
    margin: 1.25em 0;
    padding-left: 1.5em;
  }

  .prose li {
    margin-bottom: 0.5em;
  }

  .prose strong {
    color: var(--color-text-primary);
    font-weight: 600;
  }

  .prose em {
    font-style: italic;
  }

  .section-divider {
    border: none;
    border-top: 1px solid var(--color-border);
    margin: 3rem 0;
  }
</style>
