---
import BaseLayout from '../../layouts/BaseLayout.astro';
import Navigation from '../../components/Navigation.astro';
import Footer from '../../components/Footer.astro';
import BreadcrumbSchema from '../../components/schemas/BreadcrumbSchema.astro';

const breadcrumbs = [
  { name: "Home", url: "https://snailsploit.com/" },
  { name: "Frameworks", url: "https://snailsploit.com/frameworks/" }
];
---

<BaseLayout
  title="Security Frameworks | Unified Adversarial Psychology"
  description="Three frameworks, one principle. AATMF for AI threat modeling, SEF for social engineering, P.R.O.M.P.T for communication. Built on unified adversarial psychology."
  canonical="https://snailsploit.com/frameworks/"
  ogImage="/images/og-frameworks.jpg"
  keywords={[
    'AI threat modeling framework',
    'AATMF',
    'prompt engineering methodology',
    'P.R.O.M.P.T framework',
    'social engineering framework',
    'adversarial psychology'
  ]}
>
  <Navigation slot="header" />
  <BreadcrumbSchema items={breadcrumbs} />

  <article class="main-content py-12 fade-in">
    <header class="mb-12">
      <h1 class="title-hero title-gradient mb-6">Security Frameworks</h1>
      <p class="subtitle-hero">Three frameworks, one foundation</p>
    </header>

    <div class="prose prose-invert prose-lg max-w-none">
      <section class="mb-12">
        <h2 style="color: var(--color-text-primary);">Inherited Vulnerabilities</h2>
        <p>
          Every security framework I've built starts from a single observation: <strong>LLMs exhibit the same trust reflexes as humans because they learned from human-generated data.</strong>
        </p>
        <p>
          This isn't metaphor. It's a consequence of how these systems are trained.
        </p>
        <p>
          Large language models learn from billions of documents capturing how humans communicate, persuade, comply, and resist. When an LLM encounters a request framed as authority, it tends to respond the way humans respond to authority. When it sees social proof, urgency, or reciprocity, it activates compliance patterns that social engineers have exploited in humans for decades.
        </p>
        <p>
          The implication reshapes how we think about AI security: <strong>social engineering and prompt injection aren't merely analogous — they're the same attack class, executed against different substrates.</strong>
        </p>
        <p>
          This observation unifies my research. Rather than treating AI security, social engineering, and adversarial communication as separate disciplines, I approach them as applications of one underlying principle: adversarial psychology operates independently of whether the target is carbon or silicon.
        </p>
      </section>

      <hr class="section-divider" />

      <section class="mb-12">
        <h2 style="color: var(--color-text-primary);">Three Frameworks, One Foundation</h2>

        <h3 style="color: var(--color-text-primary);">AATMF — Adversarial AI Threat Modeling Framework</h3>
        <p>
          AATMF applies adversarial psychology to machine systems.
        </p>
        <p>
          The framework provides 14 tactics and 40+ techniques for systematically testing AI systems — LLMs, RAG pipelines, multimodal models, and autonomous agents. But unlike checklists that treat prompt injection as an isolated vulnerability class, AATMF maps AI attacks to the psychological principles that make them effective.
        </p>
        <p>
          Persona Override (AT-001) exploits authority compliance. Contextual Drift (AT-003) exploits gradual escalation — the same boiling-frog phenomenon that bypasses human judgment when changes are incremental. Euphemism Substitution (AT-010) exploits the linguistic reframing techniques that social engineers use to make dangerous requests sound benign.
        </p>
        <p>The framework includes:</p>
        <ul>
          <li>Quantitative risk scoring (AATMF-R)</li>
          <li>Red-Card evaluations for CI/CD integration</li>
          <li>Crosswalks to OWASP LLM Top-10, NIST AI RMF, and MITRE ATLAS</li>
        </ul>
        <p><a href="/frameworks/aatmf/">Explore AATMF →</a></p>
      </section>

      <hr class="section-divider" />

      <section class="mb-12">
        <h3 style="color: var(--color-text-primary);">SEF — Social Engineering Framework</h3>
        <p>
          SEF applies adversarial psychology to human systems.
        </p>
        <p>
          Social engineering assessments often rely on ad-hoc testing — a phishing campaign here, a vishing call there. SEF provides a structured methodology for comprehensive human-factor security assessment, grounded in the same psychological principles that AATMF applies to AI.
        </p>
        <p>The framework covers:</p>
        <p>
          <strong>Attack Taxonomy</strong> — Categorized by psychological vector (authority, social proof, urgency, reciprocity, commitment/consistency, liking) rather than delivery mechanism. A phishing email and a pretexting call that both exploit authority are functionally the same attack; SEF treats them that way.
        </p>
        <p>
          <strong>Assessment Methodology</strong> — Phased approach from reconnaissance through exploitation, with metrics for measuring organizational resilience rather than just click rates.
        </p>
        <p>
          <strong>AI-Era Adaptations</strong> — Modern social engineering increasingly uses deepfakes, voice cloning, and LLM-generated personalization. SEF includes threat matrices for these emerging vectors and assessment techniques that account for AI-augmented attacks.
        </p>
        <p>
          <strong>Quantifiable Metrics</strong> — Success rates by attack vector, role-based vulnerability profiles, and benchmarks for measuring improvement over time.
        </p>
        <p><a href="/frameworks/sef/">Explore SEF →</a></p>
      </section>

      <hr class="section-divider" />

      <section class="mb-12">
        <h3 style="color: var(--color-text-primary);">P.R.O.M.P.T — Adversarial Communication Framework</h3>
        <p>
          P.R.O.M.P.T applies adversarial psychology to communication itself.
        </p>
        <p>
          The framework began as a prompt engineering methodology, but its applications extend to any high-stakes communication where you're trying to elicit specific behavior from a system — human or artificial.
        </p>
        <ul>
          <li><strong>P</strong> — Purpose: Define the outcome before the interaction.</li>
          <li><strong>R</strong> — Results: Specify measurable success criteria.</li>
          <li><strong>O</strong> — Obstacles: Anticipate resistance and plan mitigation.</li>
          <li><strong>M</strong> — Mindset: Frame the request to align with the target's existing beliefs.</li>
          <li><strong>P</strong> — Preferences: Adapt to the target's communication style.</li>
          <li><strong>T</strong> — Technical: Handle constraints and edge cases.</li>
        </ul>
        <p>
          For AI systems, P.R.O.M.P.T structures effective prompts. For human interactions, it structures effective requests. The methodology is consistent because the underlying psychology is consistent.
        </p>
        <p><a href="/frameworks/prompt/">Explore P.R.O.M.P.T →</a></p>
      </section>

      <hr class="section-divider" />

      <section class="mb-12">
        <h2 style="color: var(--color-text-primary);">Why Unified Frameworks Matter</h2>
        <p>
          Security practitioners typically specialize. AI red teamers don't run phishing assessments. Social engineers don't audit RAG pipelines. But attackers don't observe these boundaries. A sophisticated adversary might chain a social engineering attack (compromise a developer's credentials) with a prompt injection attack (poison the knowledge base that developer accesses) with a traditional exploit (pivot from the AI system to infrastructure).
        </p>
        <p>
          Unified frameworks enable unified defense. Understanding that authority exploitation operates similarly against humans and AI systems allows you to build defenses that address root causes rather than chasing attack variants.
        </p>
        <p>My frameworks are designed to work together:</p>
        <ul>
          <li><strong>Threat modeling an AI-powered customer service system?</strong> Use AATMF for the LLM attack surface and SEF for the human operators who can override it.</li>
          <li><strong>Assessing organizational security posture?</strong> Use SEF for the human layer, AATMF for AI systems in the environment, and P.R.O.M.P.T to structure engagement communications.</li>
          <li><strong>Building AI safety controls?</strong> Map AATMF techniques to SEF psychological vectors to understand which controls address root causes versus symptoms.</li>
        </ul>
      </section>

      <hr class="section-divider" />

      <section class="mb-12">
        <h2 style="color: var(--color-text-primary);">Getting Started</h2>
        <p>
          <strong>New to adversarial AI?</strong> Start with AATMF's Key Concepts and the foundational articles in the "Start Here" section.
        </p>
        <p>
          <strong>Experienced security practitioner?</strong> Jump to AATMF's Tactics & Techniques for the full taxonomy, or explore SEF's assessment methodology.
        </p>
        <p>
          <strong>Interested in the underlying theory?</strong> Adversarial Minds provides the deep dive on the psychology that informs all three frameworks.
        </p>
        <div class="cta-links">
          <a href="/frameworks/aatmf/" class="cta-link">Explore AATMF →</a>
          <a href="/frameworks/sef/" class="cta-link">Explore SEF →</a>
          <a href="/adversarial-minds/" class="cta-link">Read Adversarial Minds →</a>
        </div>
      </section>
    </div>
  </article>

  <Footer slot="footer" />
</BaseLayout>

<style>
  .prose {
    color: var(--color-text-primary);
  }

  .prose h2 {
    font-size: 1.875rem;
    font-weight: 700;
    margin-top: 2em;
    margin-bottom: 1em;
    font-family: var(--font-mono);
  }

  .prose h3 {
    font-size: 1.5rem;
    font-weight: 600;
    margin-top: 1.75em;
    margin-bottom: 0.75em;
    font-family: var(--font-mono);
  }

  .prose p {
    margin-bottom: 1.25em;
    line-height: 1.75;
  }

  .prose a {
    color: var(--color-accent-red);
    text-decoration: underline;
  }

  .prose a:hover {
    color: var(--color-accent-green);
  }

  .prose ul {
    margin: 1.25em 0;
    padding-left: 1.5em;
  }

  .prose li {
    margin-bottom: 0.5em;
  }

  .prose strong {
    color: var(--color-text-primary);
    font-weight: 600;
  }

  .section-divider {
    border: none;
    border-top: 1px solid var(--color-border);
    margin: 3rem 0;
  }

  .cta-links {
    display: flex;
    flex-wrap: wrap;
    gap: 1.5rem;
    margin-top: 1.5rem;
  }

  .cta-link {
    color: var(--color-accent-red);
    font-weight: 600;
    text-decoration: none;
    transition: color 0.2s ease;
  }

  .cta-link:hover {
    color: var(--color-accent-green);
  }
</style>
